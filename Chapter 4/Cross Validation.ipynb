{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation in Python\n",
    "Splitting your data into training, validation, and test sets may seem straightforward, but there are a few advanced ways to do it that can come in handy when little data is available.\n",
    "\n",
    "Here, we will review **three** classic evaluation recipes:\n",
    "\n",
    "- Hold-out Validation\n",
    "- K-fold Validtation\n",
    "- Iterated K-Fold Validation with Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-Out Validation\n",
    "Defined by holding out a specified number of samples of the data set as the validation set, and never changing the validation set to any different sample of the data. This is the simplest evaluation protocol, but has the largest flaw: **if lilttle data is available, then your validation and test sets contain too few samples to be statistically representative of the data at hand.**\n",
    "\n",
    "#### Hold-out validation in Python\n",
    "```python\n",
    "\n",
    "num_validation_samples = 10000\n",
    "\n",
    "np.random.shuffle(data) # Shuffling the data is usually appropriate\n",
    "\n",
    "validation_data = data[:num_validation_samples] # Defines the validation set\n",
    "\n",
    "data = data[num_validation_samples:]\n",
    "training_data = data[:] # Defines the training set\n",
    "\n",
    "# Trains a model on the training data, and evaluates it on the validation data\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "###########################################\n",
    "# At this point you can tune your model,\n",
    "# retrain it, evaluate it, tune it again...\n",
    "###########################################\n",
    "\n",
    "# Once you’ve tuned your hyperparameters,\n",
    "# it’s common to train your final model\n",
    "# from scratch on **all** non-test data available.\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "test_score = model.evaluate(test_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Validation\n",
    "Defined by splitting the data into $K$ partitions of equal size. For each partition $i$, train a model on the remaining $K - 1$ partitions, and evaluate it on partition $i$. Your final score is then the averages of the $K$ scores obtained.\n",
    "\n",
    "**This method is helpful when the performance of your model shows significant variance based on your train-test split.**\n",
    "\n",
    "#### K-Fold Validation in Python\n",
    "\n",
    "```python\n",
    "\n",
    "k = 4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    # Selects the validation-data partition\n",
    "    validation_data = data[num_validation_samples * fold:num_validation_samples * (fold + 1)]\n",
    "    # Uses the remainder of the data as training data. Note the concatenation (+)\n",
    "    training_data = data[:num_validation_samples * fold] + \n",
    "        data[num_validation_samples * (fold + 1):]\n",
    "        \n",
    "    # Creates a brand-new instance of the model (untrained)\n",
    "    model = get_model()\n",
    "    model.train(training_data) # Trains the model\n",
    "    validation_score = model.evaluate(validation_data) # Evaluates using validation data for this iteration\n",
    "    validation_scores.append(validation_score) # Store results\n",
    "\n",
    "# Validation score: average of the validation scores of the k folds\n",
    "validation_score = np.average(validation_scores)\n",
    "    \n",
    "# Trains the final model on all non-test data available\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated K-Fold Validation with Shuffling\n",
    "**This one is for situations in which you have relatively little data available and you need to evaluate your model as precisely as possible.**  It consists of applying $K$-fold validation multiple times, shuffling the data every time before splitting it $K$ ways. The final score is the average of the scores obtained at each run of $K$-fold validation. Note that you end up training and evaluating $P \\times K$ models (where $P$ is the number of iterations you use), which can be very expensive.\n",
    "\n",
    "### Things to keep in mind\n",
    "You will want to shuffle or not depending on the attributes of your data. This is usually obvious, such as a time series data set, or if the data is ordered in a way so that a sample will not be representative of the data as a whole.\n",
    "\n",
    "**Additionally, make sure your training set and validation set are disjoint.** That is, if some data points in your data appear twice (fairly common with real-world data), then shuffling the data and splitting it into a\n",
    "training set and a validation set will result in redundancy between the training and validation sets. In effect, you’ll be testing on part of your training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
